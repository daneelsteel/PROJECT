{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "from fake_useragent import UserAgent\n",
    "from nltk import word_tokenize\n",
    "from collections import Counter\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "morph = MorphAnalyzer()\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "ua = UserAgent(verify_ssl=False)\n",
    "session = requests.session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "30"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|█████████████████████                                                               | 1/4 [00:02<00:06,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<Response [200]>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████████████████████████                                          | 2/4 [00:03<00:03,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████████████████████████████████████████████████████████████                     | 3/4 [00:04<00:01,  1.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:05<00:00,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:05<00:00,  1.50s/it]\n"
     ]
    }
   ],
   "source": [
    "news = []\n",
    "page_number = 0\n",
    "for i in tqdm(range(4)):\n",
    "    page_number += 1\n",
    "    url = f'https://www.mk.ru/incident/{page_number}/'\n",
    "    req = session.get(url, headers={'User-Agent': ua.random})\n",
    "    print(req)\n",
    "    req.encoding = 'utf-8'\n",
    "    page = req.text\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "    each_news = soup.find_all('div', {'class': 'listing-item__content'})\n",
    "    if each_news:\n",
    "        print(len(each_news))\n",
    "    for one_piece in each_news:\n",
    "        link = one_piece.find('a', {'class': 'listing-preview__content'}).attrs['href']\n",
    "        news.append(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n"
     ]
    }
   ],
   "source": [
    "print(len(news))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<p>Мать пострадавших по профессии повар, отец - дворник, оба приехали из Киргизии. «Двушку» на Зеленом проспекте они сняли год назад. Когда началась пандемия, мужчина потерял большую часть заработка, поэтому платить аренду стало тяжело. На семейном совете было решено сдать одну комнату какому-нибудь земляку.  </p>, <p>В начале сентября нашелся приезжий из Ошской области. Мужчина заявил, что он аниматор и что к нему скоро приедет жена с ребенком. Позже выяснилось, что он придумал эту историю - на самом деле он одинок.</p>, <p>Об ужасной ситуации 29-летняя мать девочек 6 и 7 лет от роду узнала совершенно случайно - 19 октября во время заучивания с детьми стихов. Старшая девочка, увидев соседа по квартире, вышедшего из комнаты в туалет, внезапно замкнулась и стала в волнении грызть ногти, заявив, что боится этого дядю. Женщина поинтересовалась происходящим, но девочка не ответила. </p>, <p>А вторая дочка, более открытая по характеру, как на духу выложила неприятные подробности. Она рассказала, как во время школьных каникул в октябре мужчина производил с ними развратные действия. В семье растет еще третья  девочка (ей 4,5 года) — ее извращенец не трогал. </p>, <p>Приставать к детям квартирант начал в начале октября, когда начались школьные каникулы. Он делал это днем, когда их отец выходил в магазин за продуктами, а мать была на работе. </p>, <p>Насильника задержали в доме на 2-ой Владимирской улице. При виде сотрудников полиции негодяй попытался подойти к окну, но путь преградили. После проведения следственных действий в здании СУ на Преображенской площади мужчина отпросился в туалет и снова попытался сбежать через окно. Он был госпитализирован с тупой травмой живота.</p>, <p>Мать малышек старается не обсуждать случившееся с дочками в надежде, что они забудут о об этом.</p>]\n"
     ]
    }
   ],
   "source": [
    "link = news[10]\n",
    "req = session.get(link, headers={'User-Agent': ua.random})\n",
    "req.encoding = 'utf-8'\n",
    "req = req.text\n",
    "req = BeautifulSoup(req, 'html.parser')\n",
    "new_text = req.find('div', {'class': \"article__body\"}) #.text.lower()\n",
    "#print(new_text)\n",
    "new_text = new_text.find_all('p') #.text.lower()\n",
    "print(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_news(link_list):\n",
    "    texts = []\n",
    "    titles = []\n",
    "    for link in tqdm(link_list):\n",
    "        news = []\n",
    "        req = session.get(link, headers={'User-Agent': ua.random})\n",
    "        req.encoding = 'utf-8'\n",
    "        req = req.text\n",
    "        req = BeautifulSoup(req, 'html.parser')\n",
    "        title = req.find('h1', {'class': \"article__title\"})\n",
    "        titles.append(title)\n",
    "        new_text = req.find('div', {'class': \"article__body\"})\n",
    "        new_text = new_text.find_all('p')\n",
    "        for p in new_text:\n",
    "            p = p.text\n",
    "            news.append(p)\n",
    "        texts.append(news)\n",
    "    return titles, texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 120/120 [02:59<00:00,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all clear!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "titles, texts = get_full_news(news)\n",
    "if len(texts)==len(titles):\n",
    "    print('all clear!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Напомним, 23-го сентября вечером дочь артиста София Конкина пришла в фитнес клуб в Пресненском районе, что неподалёку от съёмной квартиры, в которой последние несколько месяцев она жила с дочерью и своим кавалером, водителем по профессии Михаилом. Девушка сразу направилась в бассейн, где около полуночи загадочным образом утонула. В бессознательном состоянии Софию вытащили из воды и «скорая» доставила её в больницу, но спасти девушку врачам не удалось.',\n",
       "  'Смерть дочери знаменитого актера получила широкий резонанс из-за массы несостыковок и противоречивых данных об обстоятельствах, предшествующих трагедии. Например, у родственников погибшей возникли подозрения по поводу личности её сожителя, а сам факт гибели здоровой и хорошо плавающей девушки и по сей день выглядит странным. Близкие Софии до сих пор отказываются верить в несчастный случай, даже несмотря на то, что в её организме был обнаружен алкоголь и следы седативного препарата, и настаивают на полноценном расследовании (в том числе, на оценке действий сотрудников и администрации фитнес-центра) и проверки всех версий, включая криминальные. Сам Владимир Конкин обратился к председателю СКР Александру Бастрыкину, после чего уголовное дело по статье УК РФ «Об оказании услуг, не отвечающих требованиям безопасности» было передано в центральный аппарат ведомства.',\n",
       "  'Мы связались с представителем Владимира Конкина, чтобы узнать, как обстоят дела с расследованием странной гибели Софии. По словам Юлии Нитченко, как только материалы поступили в производство следственной группы ЦА СКР, дело сдвинулось с мёртвой точки.',\n",
       "  '- Следственные действия идут полным ходом. После того, как Владимира Алексеевича признали потерпевшим по делу и допросили, следователи успели пообщаться с братом Софии и помощницей Конкина. Если Святослав не очень близко общался с сестрой, то Елена пояснила, что незадолго до гибели дочери моего доверителя встречалась с ней в кафе и ничего тревожного не заметила. Помощница Владимира Алексеевича в очередной раз передала Софии конверт с деньгами (несколько десятков тысяч рублей) от отца – Конкин поддерживал дочь и регулярно помогал ей материально, - сообщила защитница.',\n",
       "  'По словам Нитченко, многое будет ясно, когда будут готовы результаты назначенных экспертиз – судебно-медицинской и судебно-компьютерной. А пока она отмечает, что с ней и с помощницей Владимира Конкина почти ежедневно связываются незнакомые им люди из различных регионов, которые сообщают им о якобы темном прошлом последнего кавалера Софии Михаила Серова. \\xa0',\n",
       "  'По нашим данным, следователи также не оставили без внимания подозрения родственников погибшей о причастности Серова к трагедии, и вызывали его на допрос аж три раза. Кроме того, от источника из близкого окружения семьи артиста нам стало известно, что родственники Софии не могут найти в вещах фамильные драгоценности, переданные Конкиной её бабушкой. Речь о дорогостоящих антикварных украшениях – серьгах с крупными бирюзовыми бриллиантами. По словам нашего собеседника, Михаил Серов в разговоре с близкими своей пассии якобы предположил, что София могла сдать украшения в ломбард, но в таком случае, по их подсчётам, девушка смогла бы безбедно существовать около года и погасить все свои задолженности по кредитам. Этого, однако,\\xa0не произошло, и о данном факте также было заявлено следствию.',\n",
       "  '\\n\\nДочь Владимира Конкина София загадочно погибла в Москве: последние фото\\n\\n\\n\\n\\n\\n\\nСмотрите фотогалерею по теме\\n\\n\\n',\n",
       "  '\\n\\nСмотрите фотогалерею по теме\\n',\n",
       "  'Что касается обнаруженных экспертами следов лекарственного средства\\xa0в организме Софии, которые стали поводом для обсуждения (в беседе с «МК» сотрудник Экспертно-криминалистического центра МВД поделился, что в практике не раз встречал комбинацию этого препарата вкупе с алкоголем, у людей самовольно ушедших из жизни), то разгадка оказалась на поверхности. Источник, знакомый с ходом расследования заверил: следователи почти уверены, что данное вещество входило в состав медикаментов, которые использовали сотрудники «скорой», когда реанимировали Софию Конкину. Помимо седативных свойств, данный препарат обладает противосудоржным и миорекласирующим действием.',\n",
       "  'Читайте также:\\xa0Раскрыты детали допроса Владимира Конкина о гибели дочери']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>titles</th>\n",
       "      <th>texts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Раскрыта тайна препарата в организме утонувше...</td>\n",
       "      <td>[Напомним, 23-го сентября вечером дочь артиста...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Девочка сбежала от матери, делавшей из нее ра...</td>\n",
       "      <td>[Наверное, у каждого были прадедушки и прабабу...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Медбрат украл аппарат УЗИ у больных коронавир...</td>\n",
       "      <td>[Как стало известно «МК», пропажу аппарата УЗИ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Составлен рейтинг мошенничеств с коронавирусом]</td>\n",
       "      <td>[Лже-медики. Облапошивание начинается со звонк...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Приговор обрадовал депутата-коммуниста Шереме...</td>\n",
       "      <td>[Ранее в ходе прений прокурор потребовал для Ш...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>[Подросток умер во время вечеринки на юго-запа...</td>\n",
       "      <td>[Как стало известно «МК», инцидент произошел в...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>[Российский полицейский убил любовницу-трансге...</td>\n",
       "      <td>[По информации издания, поначалу мужчина пытал...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>[Студент МГИМО покончил с собой из-за сессии]</td>\n",
       "      <td>[По информации агентства, его тело обнаружили ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>[Рабочего-расчленителя задержали в Петербурге]</td>\n",
       "      <td>[По версии следственных органов, 5 сентября ме...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>[Футболисты забили до смерти одноклубника из-з...</td>\n",
       "      <td>[Как пишет Daily Monitor, причиной расправы ст...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                titles  \\\n",
       "0    [Раскрыта тайна препарата в организме утонувше...   \n",
       "1    [Девочка сбежала от матери, делавшей из нее ра...   \n",
       "2    [Медбрат украл аппарат УЗИ у больных коронавир...   \n",
       "3     [Составлен рейтинг мошенничеств с коронавирусом]   \n",
       "4    [Приговор обрадовал депутата-коммуниста Шереме...   \n",
       "..                                                 ...   \n",
       "115  [Подросток умер во время вечеринки на юго-запа...   \n",
       "116  [Российский полицейский убил любовницу-трансге...   \n",
       "117      [Студент МГИМО покончил с собой из-за сессии]   \n",
       "118     [Рабочего-расчленителя задержали в Петербурге]   \n",
       "119  [Футболисты забили до смерти одноклубника из-з...   \n",
       "\n",
       "                                                 texts  \n",
       "0    [Напомним, 23-го сентября вечером дочь артиста...  \n",
       "1    [Наверное, у каждого были прадедушки и прабабу...  \n",
       "2    [Как стало известно «МК», пропажу аппарата УЗИ...  \n",
       "3    [Лже-медики. Облапошивание начинается со звонк...  \n",
       "4    [Ранее в ходе прений прокурор потребовал для Ш...  \n",
       "..                                                 ...  \n",
       "115  [Как стало известно «МК», инцидент произошел в...  \n",
       "116  [По информации издания, поначалу мужчина пытал...  \n",
       "117  [По информации агентства, его тело обнаружили ...  \n",
       "118  [По версии следственных органов, 5 сентября ме...  \n",
       "119  [Как пишет Daily Monitor, причиной расправы ст...  \n",
       "\n",
       "[120 rows x 2 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(list(zip(titles, texts)), \n",
    "               columns =['titles', 'texts']) \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('textbase.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_df = pd.DataFrame({\n",
    "   'EmployeeId': ['001', '002', '003', '004'],\n",
    "   'City': ['я хорошая и дружелюбная сорока. меня хвалят. ', 'бегает. прыгает. ', 'смешной и. расплывчатый', 'кто ты. кто я. '] \n",
    "})\n",
    "toy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_col = 'City' \n",
    "\n",
    "x = toy_df.assign(**{lst_col:toy_df[lst_col].str.split('.')})\n",
    "\n",
    "my_dataframe = pd.DataFrame({col:np.repeat(x[col].values, x[lst_col].str.len()) for col in x.columns.difference([lst_col])}).assign(**{lst_col:np.concatenate(x[lst_col].values)})[x.columns.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dataframe['words'] = my_dataframe['City']\n",
    "lst_col = 'words' \n",
    "\n",
    "x = my_dataframe.assign(**{lst_col:my_dataframe[lst_col].str.split(' ')})\n",
    "\n",
    "my_final_dataframe = pd.DataFrame({col:np.repeat(x[col].values, x[lst_col].str.len()) for col in x.columns.difference([lst_col])}).assign(**{lst_col:np.concatenate(x[lst_col].values)})[x.columns.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_final_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_final_dataframe['lemma'] = my_final_dataframe.apply(lambda row: morph.parse(row.words)[0].normal_form, axis = 1)\n",
    "my_final_dataframe['form'] = my_final_dataframe.apply(lambda row: morph.parse(row.words)[0].tag, axis = 1)\n",
    "my_final_dataframe['POS'] = my_final_dataframe.apply(lambda row: morph.parse(row.words)[0].tag.POS, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_final_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_final_dataframe.drop(my_final_dataframe[my_final_dataframe.words == ''].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_final_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOUN\tимя существительное\n",
    "\n",
    "ADJF\tимя прилагательное (полное)\n",
    "\n",
    "ADJS\tимя прилагательное (краткое)\n",
    "\n",
    "COMP\tкомпаратив\n",
    "\n",
    "VERB\tглагол (личная форма)\n",
    "\n",
    "INFN\tглагол (инфинитив)\n",
    "\n",
    "PRTF\tпричастие (полное)\n",
    "\n",
    "PRTS\tпричастие (краткое)\n",
    "\n",
    "GRND\tдеепричастие\n",
    "\n",
    "NUMR\tчислительное\n",
    "\n",
    "ADVB\tнаречие\n",
    "\n",
    "NPRO\tместоимение-существительное\n",
    "\n",
    "PRED\tпредикатив\n",
    "\n",
    "PREP\tпредлог\n",
    "\n",
    "CONJ\tсоюз\n",
    "\n",
    "PRCL\tчастица\n",
    "\n",
    "INTJ\tмеждометие"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_morphology(word):\n",
    "    word1 = word.strip('\"')\n",
    "    df_word = my_final_dataframe.loc[my_final_dataframe['words'] == word1]\n",
    "    list_cities = []\n",
    "    for index, row in df_word.iterrows():\n",
    "        answer_word = str('This word is in:'+ row['City'] + ', which comes from the article:' + row['EmployeeId'])\n",
    "        list_cities.append(answer_word)\n",
    "    return list_cities\n",
    "    \n",
    "def get_tagged(word, allowed_tags=['NOUN', 'ADJF', 'ADJS', 'COMP', 'VERB', 'INFN', 'PRTF', 'PRTS', 'GRND', 'NUMR', 'ADVB', 'NPRO', 'PRED', 'PREP', 'CONJ', 'PRCL', 'INTJ']):\n",
    "    compl_word = word.split('+')\n",
    "    tag = set(allowed_tags)&set(compl_word)\n",
    "    if not tag:\n",
    "        return ['this POS tag is not in my vocabulary. Sorry!']\n",
    "        \n",
    "    else:\n",
    "        current_word = morph.parse(compl_word[0])[0]\n",
    "        current_word_lemma = current_word.normal_form\n",
    "        current_word_POS = current_word.tag.POS\n",
    "        df_word_POS = my_final_dataframe.loc[(my_final_dataframe['lemma'] == current_word_lemma) & (my_final_dataframe['POS'] == current_word_POS)]\n",
    "        list_taggers = []\n",
    "        for index, row in df_word_POS.iterrows():\n",
    "            answer_tagger = str('This word is in:'+ row['City'] + ', which comes from the article:' + row['EmployeeId'])\n",
    "            list_taggers.append(answer_tagger)\n",
    "        return list_taggers        \n",
    "    \n",
    "def get_lemma(word):\n",
    "    word_lem_parsed = morph.parse(word)[0]\n",
    "    word_lem_lemma = word_lem_parsed.normal_form\n",
    "    df_lemma = my_final_dataframe.loc[(my_final_dataframe['lemma'] == word_lem_lemma)]\n",
    "    list_lemmas = []\n",
    "    for index, row in df_lemma.iterrows():\n",
    "        answer_lemma = str('This word is in:'+ row['City'] + ', which comes from the article:' + row['EmployeeId'])\n",
    "        list_lemmas.append(answer_lemma)\n",
    "    return list_lemmas    \n",
    "    \n",
    "def get_pos(word):\n",
    "    df_pos = my_final_dataframe.loc[(my_final_dataframe['POS'] == word)]\n",
    "    list_poses = []\n",
    "    for index, row in df_pos.iterrows():\n",
    "        answer_pos = str('This word is in:'+ row['City'] + ', which comes from the article:' + row['EmployeeId'])\n",
    "        list_poses.append(answer_pos)\n",
    "    return list_poses\n",
    "    \n",
    "def work_with_words(parsed_quiery, allowed_tags=['NOUN', 'ADJF', 'ADJS', 'COMP', 'VERB', 'INFN', 'PRTF', 'PRTS', 'GRND', 'NUMR', 'ADVB', 'NPRO', 'PRED', 'PREP', 'CONJ', 'PRCL', 'INTJ']):\n",
    "    #inputs list, outputs list\n",
    "    for word in parsed_quiery:\n",
    "        trick = word[0]\n",
    "        if trick=='\"':\n",
    "            print('word form')\n",
    "            for i in get_morphology(word):\n",
    "                print(i)\n",
    "        elif '+' in word:\n",
    "            print('lemma+POS')\n",
    "            for i in get_tagged(word):\n",
    "                print(i)\n",
    "        elif word in allowed_tags:\n",
    "            print('pos')\n",
    "            for i in get_pos(word):\n",
    "                print(i)\n",
    "        else:\n",
    "            print('lemma')\n",
    "            for i in get_lemma(word):\n",
    "                print(i)\n",
    "    \n",
    "def work_with_quiery(quiery):\n",
    "    #inputs input string, outputs dataframe\n",
    "    quiery_parsed = quiery.split(' ')\n",
    "    if len(quiery_parsed)>2 or len(quiery_parsed)==0:\n",
    "        print('Oh, come on, we talked about it!')\n",
    "    else:\n",
    "        work_with_words(quiery_parsed, allowed_tags=['NOUN', 'ADJF', 'ADJS', 'COMP', 'VERB', 'INFN', 'PRTF', 'PRTS', 'GRND', 'NUMR', 'ADVB', 'NPRO', 'PRED', 'PREP', 'CONJ', 'PRCL', 'INTJ'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed_tags=['NOUN', 'ADJF', 'ADJS', 'COMP', 'VERB', 'INFN', 'PRTF', 'PRTS', 'GRND', 'NUMR', 'ADVB', 'NPRO', 'PRED', 'PREP', 'CONJ', 'PRCL', 'INTJ']\n",
    "def main():\n",
    "    quiery = input('What will we search for? ')\n",
    "    answer = work_with_quiery(quiery)\n",
    "    answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
